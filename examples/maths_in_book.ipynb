{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Tree Boosting Algorithm\n",
    "***\n",
    "1. Initialize model with a constant value <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &emsp; &emsp;${f_0}(x) = $ arg &ensp; \n",
    "   $min_{\\gamma} \\sum \\limits _{i=1} ^{N} L(y_i,{\\gamma}) $\n",
    "2. For i = 1 to M\n",
    "<br>\n",
    "         (a) For i = 1,2,..., N compute\n",
    "   <br>\n",
    "                       &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &emsp; &emsp;  $r_{im}  = -\\Bigg[\\frac{\\delta L(y_i,f(x_i))} {\\delta f(x_i)} \\Bigg]_{f=f_{m-1}}$\n",
    "                       <br>\n",
    "          (b) Fit a regression tree to the targets $r_{im}$ giving terminal regions\n",
    "               <br>\n",
    "               &emsp;&emsp;&emsp;$R_{jm}, j = 1,2,..., J_m$\n",
    "               <br>\n",
    "           (c) For $j  = 1,2,...,J_m.$ compute\n",
    "           <br>\n",
    "           &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &emsp; &emsp;$\\gamma_{jm} = arg \\ \\min \\limits_{\\gamma} \\  \\sum \\limits_{x_i \\subseteq R_{jm}} L(y_i,f_{m-1} (x_i) + \\gamma)$\n",
    "           <br><br>\n",
    "           (d) Update $f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{j_m} \\gamma_{jm}I(x \\subseteq R_{jm})$\n",
    "\n",
    "   <br>\n",
    "3. Output $ \\hat {f} = f_M(x)$\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance Covariance Matrix in Markdown\n",
    "***\n",
    "$\\Large \\Sigma = \\begin{bmatrix} \\sigma^{2}_{xx} & \\sigma_{xy} \\\\ \\sigma_{yx}\n",
    "         & \\sigma^{2}_{yy} \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^n + y^n = 2^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sqrt{x^2 + 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$     $\\beta \\gamma \\rho \\sigma \\delta \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Operators  $\\times \\  \\otimes \\ \\oplus \\ \\cup  \\ \\cap$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation Operators $< > \\ \\subset \\ \\supset \\ \\subseteq \\ \\supseteq$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greek Alphabets\n",
    "***<br>\n",
    "    Alpha $\\alpha$***<br>\n",
    "    Beta  $\\beta$\n",
    "    <br>\n",
    "    $\\gamma \\ \\Gamma$&emsp;&emsp;&emsp;&emsp;    \\gamma \\Gamma <br>\n",
    "    $\\delta \\ \\Delta$&emsp;&emsp;&emsp;&emsp;    \\delta \\Delta <br>\n",
    "    $\\epsilon \\ \\varepsilon \\ E $ &emsp;&emsp;&emsp;    \\epsilon \\varepsilon E<br>\n",
    "    $\\zeta \\ Z \\ $  &emsp; &emsp; &emsp;\\zeta Z  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others $\\int \\ \\oint \\ \\sum \\  \\prod$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superscripts and Subscripts\n",
    "<br><br>\n",
    "$\\int \\limits_0^1 x^2 + y^2 \\ dx $\n",
    "<br><br>\n",
    "Without the limits Operators\n",
    "<br><br>\n",
    "$\\int_0^1 x^2 + y^2 \\    dx $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Detailed Example <br> <br>\n",
    "$ \\ $\n",
    "$a_1^2 + a_2^2  = a_3^2$\n",
    "<br>\n",
    "The use of braces for long superscripts and subscripts\n",
    "<br>\n",
    "$x^{2 \\alpha} - 1 = y_{ij} + y_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operators Using Superscripts of Subscripts\n",
    "<br><br>\n",
    "$\\sum\\limits_{i=1}^{\\infty} \\frac{1}{n^s}  = \\prod_p \\frac{1}{1 - p^{-s}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For m = 1 to M:\n",
    "   (a) for i = 1,2, ..., N compute\n",
    "                                    $$r_{im} = - \\displaystyle \\Bigg[ \\frac{1}2 \\Bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.overleaf.com/learn/latex/Subscripts_and_superscripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPR MATHS\n",
    "***\n",
    "1. The Model\n",
    "    <br>\n",
    "    $\\vec y = \\sum \\limits_{j=1}^r f_j(\\vec x_j . \\vec \\alpha_j) \\otimes \\vec \\beta_j ^T $\n",
    "    <br><br>\n",
    "    Where;\n",
    "    <br>\n",
    "    - i iterates examples, the rows of input and output matrices <br>\n",
    "    - j iterates the number of terms in the PPR “additive model” <br>\n",
    "    - r is the total number of projections and functions (terms) in the PPR\n",
    "    -  $\\vec y_i \\ $ is a d-dimensional vector, the ith row in an output matrix \n",
    "    $Y \\subseteq \\mathbb{R^{n \\times  d}}$\n",
    "    - $\\vec x_i \\ $ is a p-dimensional vector, the ith row of an input matrix $X \\subseteq \\mathbb{R^{n \\ \\times \\ p}}$\n",
    "    - $\\alpha_j$ is the ${j}^{th}$ projection vector in the mdoel, a p-dimensional vector inner-producted with $\\vec x_i$\n",
    "    - ${f_j}$ is the jth function in the model, mapping from $\\mathbb{R^1} \\rightarrow \\mathbb{R^1} $\n",
    "    - $\\vec \\beta_j^T$ is the transpose of $\\vec \\beta_j^T$ , a d-dimensional vector outer-producted with the result of ${F_J}$ to yield a result in the output space\n",
    "    - . is an inner product\n",
    "    - $\\otimes$ is an outer product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be vectorized as <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $\\hat Y = \\sum \\limits_{j=1}^r \n",
    "    {f_j}(X . \\vec \\alpha_j) \\otimes \\vec {\\beta_j}^T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Loss Function \n",
    "***\n",
    "The supervised learning consists of minimizing a standard quadratic loss function\n",
    "<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "loss = $\\sum \\limits_{i=1}^n {w_i}({})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steepest Descent\n",
    "***\n",
    "<br> <br>\n",
    "$g_m = \\{g_{jm}\\} = \\Bigg \\{ \\Bigg [  \\frac{\\delta \\Phi(P)} {\\delta P_j} \n",
    "\\Bigg ]_{P=P_{m-1}}\n",
    "    \\Bigg \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Numerical Optimization in function space\n",
    "  ***\n",
    "  This is a non-parametric approach and apply numerical optimization in function space\n",
    "that is we consider the F(x) evalluated at each point x to be a \"parameter\" and seek to minimize\n",
    "<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "   $\\Phi(F)  = E_{y,x} L(y,F(x))  = E_x[E_y(L(y,F(x)) | x)]$,\n",
    "   <br>\n",
    "or equivalently\n",
    "<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "   $\\phi(F(x)) = E_y[L(y,F(x)) | x]$\n",
    "   <br>\n",
    "   at each individual x, directly with respect to F(x). In function space there are\n",
    "an infinite number ofsuch parameters, but in data sets (discussed below) only\n",
    "a finite number $\\{F(x_i)\\}_1^N$\n",
    "are involved. Following the numerical optimization\n",
    "paradigm we take the solution to be <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "$F*(x) = \\sum \\limits_{m=0}^M f_m(x)$\n",
    "<br>\n",
    "where $f_0(x)$ is an initial guess, and $\\{f_m(x)\\}_1^M$ are incremental functions\n",
    "(\"steps\" or \"boost\") defined by the optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Steepest Descent\n",
    "<br>\n",
    "$f_m(m) = -p_m g_m(x)$   equation 6\n",
    "<br>\n",
    "with,<br>\n",
    "&emsp;&emsp;  $g_m(x) = \\Bigg [\\ \\frac{\\delta \\phi(F(x))} {\\delta F(X)} \\Bigg]_{F(x) = F_{m-1(x)}} = \n",
    "        \\Bigg [ \\frac{\\delta E_y[L(y,F(x)) | x]} {\\delta F(x)}\\Bigg]_{F(x) = F_{m-1(x)}}$ <br>\n",
    "        and <br>\n",
    "&emsp;&emsp; <br>\n",
    "        $F_{m-1}(x)  = \\sum \\limits_{i=0}^{m-1} f_i(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://svitla.com/blog/math-function-optimization-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line Search\n",
    "https://en.wikipedia.org/wiki/Line_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming sufficient regularity that one can interchange differentiation and\n",
    "integration, this becomes <br>\n",
    "&emsp;&emsp; equation 7 &emsp;&emsp;&emsp;&emsp;\n",
    "$g_m(x)  = E_y \\Bigg [ \\frac{\\delta L(y,F(x))} {\\delta F(x)} | x\\Bigg ]_{F(x) = F_{m-1}(x)}$\n",
    "<br>\n",
    "The multiplier $p_m$ in equation 6 is given by the line search<br>\n",
    "equation 8 &emsp; &emsp;&emsp;&emsp;\n",
    "        $p_m = arg \\  min_p \\  E_{y,x} L(y, F_{m-1}(x) - p g_m(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finitedata. This nonparametric approach breaks down when the joint\n",
    "distribution of (y,x) is estimated by a finite data sample $\\{ y_i, x_i\\}_1^N$\n",
    "1 . In this\n",
    "case $E_y[. |x]$ cannot be estimated accurately by its data value at each $x_i$, and\n",
    "even ifit could, one would like to estimate $F^*(x)$ at x values other than the\n",
    "training sample points. Strength must be borrowed from nearby data points\n",
    "by imposing smoothness on the solution. One way to do this is to assume a\n",
    "parameterized form such as (2) and do parameter optimization as discussed in\n",
    "Section 1.1 to minimize the corresponding data based estimate ofexpected loss,\n",
    "<br>\n",
    "&emsp;&emsp;&emsp;&emsp;\n",
    "    $\\{\\beta_m, a_m \\}_1^M  = arg \\ \\min \\limits_{\\{\\beta_m^\\prime, a_m^\\prime\\}_1^M}\n",
    "       \\sum \\limits_{i=1}^N L \\Bigg ( y_i, \\beta_m^\\prime h(x_i;a_m^\\prime)\\Bigg) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situations where this is infeasible one can try a “greedy stagewise”\n",
    "approach. <br>\n",
    "Equation 9 &emsp;&emsp;&emsp; $(\\beta_m, \\textbf a_m) = \n",
    "    arg \\ \\min \\limits_{b,a} \\sum \\limits_{i=1}^N L(y_i, F_{m-1}(\\textbf x_i;\\textbf a))$\n",
    "    <br>\n",
    "    and then\n",
    "    <br>\n",
    "    Equation 10 &emsp;&emsp;&emsp;&emsp;\n",
    "        $F_m(\\textbf x)  = F_{m-1} + \\beta_m h(\\textbf x; \\textbf a_m)$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In signal processing this stagewise strategy is called “matching pursuit”\n",
    "[Mallat and Zhang (1993)] where L(y,f)is squared-error loss and the\n",
    "$\\{h(x;a_m) \\}_1^M$ are called basis functions, usually taken from an overcomplete\n",
    "waveletlike dictionary. In machine learning, (9), (10) is called “boosting” where\n",
    "$y \\subseteq \\{ -1, 1\\}$ and L(y,f) is either an exponential loss criterion $e^{-yf}$ [Freund\n",
    "and Schapire (1996), Schapire and Singer (1998)] or negative binomial log-\n",
    "likelihood [Friedman, Hastie and Tibshirani (2000) (here after reffered to as\n",
    "FHT00)]. The function h(x;a) is called a “weak learner” or “base learner” and\n",
    "is usually a classification tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that for a particular loss L(y,F ) and/or  base learner h(x;a) the\n",
    "solution to (equation 9) is difficult to obtain. Given any approximator $F_{m-1}(x)$, the\n",
    "function $\\beta_m h(x;a_m)$ (9), (10) can be viewed as the best greedy step toward\n",
    "the data-based estimate of $F^*(x)$ (1), under the constraint that the step “direction” $h(x;a_m)$ be a member ofthe parameterized class offunctions h(x;a). It\n",
    "can thus be regarded as a steepest descent step (6) under that constraint. By construction, the data-based analogue ofthe unconstrained negative gradient (7) <br>\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp; \n",
    "            $-g_m(x_i) = - \\Bigg[frac{\\delta L(y_i,F(x_i))} {\\delta F(x_i)}  \\Bigg]_\n",
    "            {F(x)=F_{m-1}(x)}$\n",
    "            <br>\n",
    "            gives the best steepest-descent step direction $-g_m = \\{-g_m(x_i) \\}_1^N$\n",
    "in the N-dimensional data space at $F_{m-1}(x)$. However, this gradient is defined only at\n",
    "the data points $\\{x_i\\}_1^N$\n",
    "1 and cannot be generalized to other x-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possibility for generalization is to choose that member of the parameterized class \n",
    "$h(x;a_m)$ that produces $h_m = \\{h(x_i;a_m)\\}_1^N$ most parallel to $-g_m \\subseteq \\mathbb{R^N}$. This is the h(x;a) most highly correlated with $-g_m(x)$ over the data distribution. It can be obtained from the solution <br>\n",
    "Equation (11) &emsp;&emsp;&emsp;&emsp; $a_m = arg \\min \\limits_{a,\\beta}\n",
    "    \\sum \\limits_{i=1}^N [-g_m(x_i) - \\beta h(x_i;a)]^2$ <br>\n",
    "    This constrained negative gradient $h(x;a_m)$ is used in place of the uncon-\n",
    "strained one $-g_m(x)$ (7) in the steepest-descent strategy. Specifically, the line\n",
    "search (8) is performed <br>\n",
    "equation (12) &emsp;&emsp;&emsp;&emsp;\n",
    "        $p_m = arg \\min \\limits_p \\sum \\limits_{i=1}^N L(y_i,F_{m-1}(x_i) + ph(x_i;a_m))$ <br>\n",
    "        and the approximation updated, <br>\n",
    "$F_m(x) = F_{m-1}(x) + p_mh(x_i;a_m)$ <br>\n",
    "Basically, instead ofobtaining the solution under a smoothness constraint\n",
    "(9), the constraint is applied to the unconstrained (rough) solution by fitting h(x;a) to the “pseudoresponses”  $\\{\\stackrel{\\sim}{y_i} = -g_m(x_i)\\}_{i=1}^N $ (equation 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This permits the replacement of the difficult function minimization problem (9) by least-squares\n",
    "function minimization (11), followed by only a single parameter optimization\n",
    "based on the original criterion (12). Thus, for any h(x;a) for which a feasible\n",
    "least-squares algorithm exists for solving (11), one can use this approach to\n",
    "minimize any differentiable loss L(y,F) in conjunction with forward stage-\n",
    "wise additive modeling. This leads to the following (generic) algorithm using\n",
    "steepest-descent. <br>\n",
    "Algorithm 1 (Gradient Boost) <br>\n",
    "1. &emsp;&emsp; $F_0  = arg \\min_p \\sum_{i=1}^N L(y_i,p)$ <br>\n",
    "2. &emsp;&emsp; For m =  1 do: <br>\n",
    "3. &emsp;&emsp; $\\{\\stackrel{\\sim}{y_i} = -[\\frac{\\delta L(y_i, F(x_i))} {\\delta F(x_i)}\n",
    "    ]_F(x)=F_{m-1}(x), \\  \\  i = 1,N$ <br>\n",
    "4. &emsp;&emsp; $a_m = arg \\min_{a,\\beta} \\sum_{i=1}^N [\\stackrel{\\sim}{y_i} - \\beta h(x_i;a)]^2$ <br>\n",
    "5. &emsp;&emsp; $p_m = arg \\min_p \\sum_{i=1}^N L(y_i,F_{m-1}(x_i) + ph(x_i;a_m)))$ <br>\n",
    "6. &emsp;&emsp; $F_m(x) = F_{m-1} (x) + p_m h(x;a_m)$ <br>\n",
    "7. &emsp;&emsp; endFor <br>\n",
    "&emsp;&emsp;&emsp;&emsp; end algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that any fitting criterion that estimates conditional expectation (given\n",
    "x) could in principle be used to estimate the (smoothed) negative gradient (7)\n",
    "at line 4 ofAlgorithm 1. Least-squares (11) is a natural choice owing to the\n",
    "superior computational properties ofmany least-squares algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the special case where $y \\subseteq \\{-1,1 \\}$ and the loss function  depends\n",
    "on y and F only through their product L\u0004y\u0004 F\u0005 = L\u0004yF\u0005, the analogy ofboost-\n",
    "ing (9), (10) to steepest-descent minimization has been noted in the machine\n",
    "learning literature [Ratsch, Onoda and Muller (1998), Breiman (1999)]. Duffy\n",
    "and Helmbold (1999) elegantly exploit this analogy to motivate their GeoLev\n",
    "and GeoArc procedures. The quantity yF is called the “margin” and the\n",
    "steepest-descent is performed in the space of margin values, rather than the\n",
    "space of function values F. The latter approach permits application to more\n",
    "general loss functions where the notion of margins is not apparent. Drucker\n",
    "(1997) employs a different strategy of casting regression into the framework of\n",
    "classification in the context ofthe AdaBoost algorithm [Freund and Schapire\n",
    "(1996)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Least-squares regression. Here $ L(y,F) = (y - F)^2 /2$  pseudoresponse in line 3 ofAlgorithm 1 is $\\stackrel{\\sim}{y_i}  = \\stackrel{\\sim}{y_i - F_{m - 1} (x_i)}$. Thus, line 4 simply fits the current residuals and the line search (line 5) produces the result $p_m = \\beta_m \\ where \\beta_m$ where $\\beta_m$ is the minimizing $\\beta$ of line 4. Therefore, gradient boosting on squared-error loss produces the usual stagewise approach ofiteratively fitting the current residuals\n",
    "<br>\n",
    "ALGORITHMN 2 (LS Boost) <br>\n",
    " $ F_0(x) = \\stackrel{\\sim}{y_i}$ <br>\n",
    "For m = 1 to M do: <br>\n",
    "&emsp;&emsp;&emsp;&emsp; $ \\stackrel{\\sim}{y_i} = y_i - F_{m - 1}(x_i), i  = 1,N $ <br>\n",
    "&emsp;&emsp;&emsp;&emsp; $(p_m,a_m)  = arg \\min_{a,p} \\sum_{i=1}^N [\\stackrel{\\sim}{y_i}\n",
    "    - ph(x_i;a)]^2$ <br>\n",
    "&emsp;&emsp;&emsp;&emsp; $F_m(x) = F_{m-1} + p_mh(x;a_m)$ <br>\n",
    "endFor <br>\n",
    "end Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MY APPEN URL\n",
    "https://ui.appen.com.cn/user/login?redirect=aHR0cHM6Ly91aS5hcHBlbi5jb20uY24vdjMvd29ya2VyLXByb2plY3RzP2FjdGl2ZU1lbnU9Y3VycmVudA=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. Least absolute deviation (LAD) regression. For the loss function $L(y,F) = | y - F|$ one has\n",
    "<br>\n",
    "\n",
    "equation 13 &emsp;&emsp;&emsp;&emsp;\n",
    "    $\\stackrel{\\sim}{y_i} = - \\Bigg[\\frac{\\delta L(y_i, F(x_i)) }{\\delta F(x_i)}  \\Bigg \n",
    "    ]_{F(x) = F_{m-1}(x)}  = sign (y_i - F_{m - 1}(x_i))$ <br>\n",
    "\n",
    "    This implies that h(x;a) is fit (by least-squares) to the sign of the current\n",
    "residuals in line 4 ofAlgorithm 1. The line search (line 5) becomes <br>\n",
    "&emsp;&emsp;&emsp;&emsp;\n",
    "    $p_m  = arg \\min \\limits_p \\sum \\limits_{i=1}^N |y_i - F_{m-1}- ph(x_i;a_m) |$ <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;\n",
    "    equation 14  = $arg \\min \\limits_p \\sum \\limits_{i=1}^N |h(x_i;a_m)| . \\Bigg | \\frac{y_i - F_{M-1}(x_i)} {h(x_i;a_m)} - p \\Bigg |$ <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;\n",
    "     = $median_w \\Bigg \\{ \\frac{y_i - F_{m-1}(x_i)}{h(x_i;a_m)}   \\Bigg \\}_1^N,   \n",
    "        w_i = |h(x_i;a_m)|$ <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "Here $median_w \\{.\\}$ is the weighted median with weights $w_i$. Inserting these\n",
    "results [(13), (14)] into Algorithm 1 yields an algorithm for least absolute\n",
    "deviation boosting, using any base learner h(x;a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAD codes\n",
    "https://github.com/giacobello/GroupSparseLP_dereverberation/tree/master/LAD\n",
    "https://github.com/mirca/lad/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3. Regression trees. Here we consider the special case where each base\n",
    "learner is an J-terminal node regression tree [Breiman, Friedman, Olshen\n",
    "and Stone (1983)]. Each regression tree model itself has the additive form <br>\n",
    "\n",
    "equation 15 &emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "    $h(\\textbf x;\\{ b_j, R_j\\}_1^J) =  \\sum \\limits_{j=1}^J 1(\\textbf x \\subseteq R_j)$<br>\n",
    "    Here $\\{ R_j\\}_1^J$ are disjoint regions that collectively cover the space of all joint\n",
    "values ofthe predictor variables x. These regions are represented by the terminal nodes ofthe corresponding tree. The indicator function $1(.)$ has the value 1 if its argument is true, and zero otherwise. The “parameters” of this base learner (15) are the coefficients $\\{ b_j\\}_1^J$\n",
    ", and the quantities that define the boundaries of the regions $\\{ R_j\\}_1^J$ . These are the splitting variables and the values of those variables that represent the splits at the nonterminal nodes of the tree. Because the regions are disjoint, (15) is equivalent to the prediction rule: $x \\subseteq R_j then \\  h(x) = b_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a regression tree, the update at line 6 ofAlgorithm 1 becomes <br>\n",
    "(16) &emsp;&emsp;&emsp;&emsp;\n",
    "$F_m(x) = F_{m-1}(\\textbf x) + p_m \\sum \\limits_{j=1}^J \n",
    "    b_{jm}1(x \\subseteq R_{jm})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
