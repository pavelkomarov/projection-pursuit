<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>skpp &#8212; projection-pursuit 1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=8cf5aea1" />
    <script src="../_static/documentation_options.js?v=f2a433a1"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for skpp</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.interpolate</span><span class="w"> </span><span class="kn">import</span> <span class="n">UnivariateSpline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">,</span> \
	<span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">MultiOutputMixin</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.exceptions</span><span class="w"> </span><span class="kn">import</span> <span class="n">NotFittedError</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils.validation</span><span class="w"> </span><span class="kn">import</span> <span class="n">check_X_y</span><span class="p">,</span> <span class="n">check_is_fitted</span><span class="p">,</span> <span class="n">column_or_1d</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils.multiclass</span><span class="w"> </span><span class="kn">import</span> <span class="n">unique_labels</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">as_float_array</span><span class="p">,</span> <span class="n">check_random_state</span><span class="p">,</span> <span class="n">check_array</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OneHotEncoder</span>

<div class="viewcode-block" id="ProjectionPursuitRegressor">
<a class="viewcode-back" href="../skpp.html#skpp.ProjectionPursuitRegressor">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ProjectionPursuitRegressor</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span>
	<span class="n">MultiOutputMixin</span><span class="p">):</span>
<span class="w">	</span><span class="sd">&quot;&quot;&quot;This class implements the PPR algorithm as detailed in math.pdf.</span>

<span class="sd">	Parameters</span>
<span class="sd">	----------</span>
<span class="sd">	r : int, default=10</span>
<span class="sd">		The number of terms in the underlying additive model. The input will be</span>
<span class="sd">		put through `r` projections, `r` functions of those projections, and</span>
<span class="sd">		then multiplication by `r` output vectors to determine output.</span>

<span class="sd">	fit_type : {&#39;polyfit&#39;, &#39;spline&#39;}, default=&#39;polyfit&#39;</span>
<span class="sd">		The kind of function to fit at each stage.</span>

<span class="sd">	degree : int, default=3:</span>
<span class="sd">		The degree of polynomials or spline-sections used as the univariate</span>
<span class="sd">		approximator between projection and weighted residual targets.</span>

<span class="sd">	opt_level : {&#39;high&#39;, &#39;medium&#39;, &#39;low&#39;}, default=&#39;high&#39;</span>
<span class="sd">		&#39;low&#39; opt_level will disable backfitting. &#39;medium&#39; backfits previous</span>
<span class="sd">		2D functional fits only (not projections). &#39;high&#39; backfits everything.</span>

<span class="sd">	example_weights : string or array-like of dimension (n_samples,), default=&#39;uniform&#39;</span>
<span class="sd">		The relative importances given to training examples when calculating</span>
<span class="sd">		loss and solving for parameters.</span>

<span class="sd">	out_dim_weights : string or array-like, default=&#39;inverse-variance&#39;</span>
<span class="sd">		The relative importances given to output dimensions when calculating the</span>
<span class="sd">		weighted residual (output of the univariate functions f_j). If all</span>
<span class="sd">		dimensions are of the same importance, but outputs are of different</span>
<span class="sd">		scales, then using the inverse variance is a good choice.</span>
<span class="sd">		Possible values: `&#39;inverse-variance&#39;`: Divide outputs by their variances,</span>
<span class="sd">		`&#39;uniform&#39;`: Use a vector of ones as the weights, `array`: Provide a custom</span>
<span class="sd">		vector of weights of dimension (n_outputs,)</span>

<span class="sd">	eps_stage : float, default=0.0001</span>
<span class="sd">		The mean squared difference between the predictions of the PPR at</span>
<span class="sd">		subsequent iterations of a &quot;stage&quot; (fitting an f, beta pair) must reach</span>
<span class="sd">		below this epsilon in order for the stage to be considered converged.</span>

<span class="sd">	eps_backfit : float, default=0.01</span>
<span class="sd">		The mean squared difference between the predictions of the PPR at</span>
<span class="sd">		subsequent iterations of a &quot;backfit&quot; must reach below this epsilon in</span>
<span class="sd">		order for backfitting to be considered converged.</span>

<span class="sd">	stage_maxiter : int, default=100</span>
<span class="sd">		If a stage does not converge within this many iterations, end the loop</span>
<span class="sd">		and move on. This is useful for divergent cases.</span>

<span class="sd">	backfit_maxiter : int, default=10</span>
<span class="sd">		If a backfit does not converge withint this many iterations, end the</span>
<span class="sd">		loop and move on. Smaller values may be preferred here since backfit</span>
<span class="sd">		iterations are expensive.</span>

<span class="sd">	random_state : int, numpy.RandomState, default=None</span>
<span class="sd">		An optional object with which to seed randomness.</span>

<span class="sd">	show_plots : boolean, default=False</span>
<span class="sd">		Whether to produce plots of projections versus residual variance</span>
<span class="sd">		throughout the training process.</span>

<span class="sd">	plot_epoch : int, default=50:</span>
<span class="sd">		If plots are displayed, show them every `plot_epoch` iterations of the</span>
<span class="sd">		stage-fitting process.</span>
<span class="sd">	</span>
<span class="sd">	&quot;&quot;&quot;</span>
	<span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fit_type</span><span class="o">=</span><span class="s1">&#39;polyfit&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">opt_level</span><span class="o">=</span><span class="s1">&#39;high&#39;</span><span class="p">,</span>
				 <span class="n">example_weights</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">out_dim_weights</span><span class="o">=</span><span class="s1">&#39;inverse-variance&#39;</span><span class="p">,</span>
				 <span class="n">eps_stage</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">eps_backfit</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">stage_maxiter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
				 <span class="n">backfit_maxiter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_plots</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
				 <span class="n">plot_epoch</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>

		<span class="c1"># paranoid parameter checking to make it easier for users to know when</span>
		<span class="c1"># they have gone awry and to make it safe to assume some variables can</span>
		<span class="c1"># only have certain settings</span>
		<span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
			<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;r must be an int &gt;= 1.&#39;</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">fit_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;polyfit&#39;</span><span class="p">,</span> <span class="s1">&#39;spline&#39;</span><span class="p">]:</span>
			<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;fit_type &#39;</span> <span class="o">+</span> <span class="n">fit_type</span> <span class="o">+</span> <span class="s1">&#39; not supported&#39;</span><span class="p">)</span>
		<span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">degree</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
			<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;degree must be &gt;= 1.&#39;</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">opt_level</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;low&#39;</span><span class="p">,</span> <span class="s1">&#39;medium&#39;</span><span class="p">,</span> <span class="s1">&#39;high&#39;</span><span class="p">]:</span>
			<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;opt_level must be either low, medium, or high.&#39;</span><span class="p">)</span>
		<span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">example_weights</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">example_weights</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">):</span>
			<span class="k">try</span><span class="p">:</span>
				<span class="n">example_weights</span> <span class="o">=</span> <span class="n">as_float_array</span><span class="p">(</span><span class="n">example_weights</span><span class="p">)</span>
			<span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;example_weights must be `uniform`, or array-like.&#39;</span><span class="p">)</span>
			<span class="k">if</span> <span class="n">numpy</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">example_weights</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;example_weights can not contain negatives.&#39;</span><span class="p">)</span>
		<span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">out_dim_weights</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">out_dim_weights</span> <span class="ow">in</span>
			<span class="p">[</span><span class="s1">&#39;inverse-variance&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">]):</span>
			<span class="k">try</span><span class="p">:</span>
				<span class="n">out_dim_weights</span> <span class="o">=</span> <span class="n">as_float_array</span><span class="p">(</span><span class="n">out_dim_weights</span><span class="p">)</span>
			<span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;out_dim_weights must be either &#39;</span> <span class="o">+</span> \
					<span class="s1">&#39;inverse-variance, uniform, or array-like.&#39;</span><span class="p">)</span>
			<span class="k">if</span> <span class="n">numpy</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">out_dim_weights</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;out_dim_weights can not contain negatives.&#39;</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">eps_stage</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">eps_backfit</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
			<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Epsilons must be &gt; 0.&#39;</span><span class="p">)</span>
		<span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stage_maxiter</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">stage_maxiter</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> \
			<span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">backfit_maxiter</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">backfit_maxiter</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
			<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Maximum iteration settings must be ints &gt; 0.&#39;</span><span class="p">)</span>

		<span class="c1"># Save parameters to the object</span>
		<span class="n">params</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
			<span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s1">&#39;self&#39;</span><span class="p">:</span>
				<span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

<div class="viewcode-block" id="ProjectionPursuitRegressor.transform">
<a class="viewcode-back" href="../skpp.html#skpp.ProjectionPursuitRegressor.transform">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Find the projections of X through all alpha vectors in the PPR.</span>

<span class="sd">		:math:`A` is a p x r matrix with projection vectors in each column, and</span>
<span class="sd">		:math:`X` is an n x p matrix with examples in each row, so the inner</span>
<span class="sd">		product of the two stores projections.</span>

<span class="sd">		Parameters</span>
<span class="sd">		----------</span>
<span class="sd">		X : array-like of shape (n_samples, n_features)</span>
<span class="sd">			The input samples.</span>

<span class="sd">		Returns</span>
<span class="sd">		-------</span>
<span class="sd">		Projections : an array of shape (n_samples, r)</span>
<span class="sd">			where r is the hyperparameter given to the constructor, the number</span>
<span class="sd">			of terms in the additive model, and the jth column is the projection</span>
<span class="sd">			of :math:`X` through :math:`\\alpha_j`.</span>
<span class="sd">		</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_alpha_&#39;</span><span class="p">)</span>
		<span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha_</span><span class="p">)</span></div>


<div class="viewcode-block" id="ProjectionPursuitRegressor.predict">
<a class="viewcode-back" href="../skpp.html#skpp.ProjectionPursuitRegressor.predict">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Use the fitted estimator to make predictions on new data.</span>

<span class="sd">		Parameters</span>
<span class="sd">		----------</span>
<span class="sd">		X : array-like of shape (n_samples, n_features)</span>
<span class="sd">			The input samples.</span>

<span class="sd">		Returns</span>
<span class="sd">		-------</span>
<span class="sd">		Y : array of shape (n_samples) or (n_samples, n_outputs)</span>
<span class="sd">			The result of passing X through the evaluation function.</span>
<span class="sd">		</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="c1"># Check whether the PPR is trained, and if so get the projections.</span>
		<span class="n">P</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># P is an n x r matrix.</span>
		<span class="c1"># Take f_j of each projection, yielding a vector of shape (n,); take the</span>
		<span class="c1"># outer product with the corresponding output weights of shape (d,); and</span>
		<span class="c1"># take the weighted sum over all terms. This is a vectorized version of</span>
		<span class="c1"># the evaluation function. </span>
		<span class="n">Y</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">numpy</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_f_</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">P</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
			<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">)])</span>
		<span class="c1"># return single-dimensional output if Y has only one column</span>
		<span class="k">return</span> <span class="n">Y</span> <span class="k">if</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">Y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="ProjectionPursuitRegressor.fit">
<a class="viewcode-back" href="../skpp.html#skpp.ProjectionPursuitRegressor.fit">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Train the model.</span>

<span class="sd">		Parameters</span>
<span class="sd">		----------</span>
<span class="sd">		X : array-like of shape (n_samples, n_features)</span>
<span class="sd">			The training input samples.</span>
<span class="sd">		Y : array-like, shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">			The target values.</span>

<span class="sd">		Returns</span>
<span class="sd">		-------</span>
<span class="sd">		self : ProjectionPursuitRegressor</span>
<span class="sd">			A trained model.</span>
<span class="sd">		</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="c1"># some stuff to make this jazz pass sklearn&#39;s check_estimator()</span>
		<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">multi_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">Y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># warning the user should pass 1D</span>
			<span class="n">Y</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">warn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># necessary to pass sklearn checks</span>
		<span class="k">if</span> <span class="n">Y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># standardize Y as 2D so the below always works</span>
			<span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># reshape returns a view to existing data</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">n_features_in_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

		<span class="c1"># Due to the enormous number of least squares fits happening here,</span>
		<span class="c1"># numerical drift is unavoidable, so short-circuit for idempotence</span>
		<span class="n">idempotence_hash</span> <span class="o">=</span> <span class="nb">hash</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">tobytes</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>
			<span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">Y</span><span class="o">.</span><span class="n">tobytes</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">Y</span><span class="p">)))</span>
		<span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;idempotence_hash_&#39;</span><span class="p">)</span> <span class="ow">and</span> \
			<span class="bp">self</span><span class="o">.</span><span class="n">idempotence_hash_</span> <span class="o">==</span> <span class="n">idempotence_hash</span><span class="p">:</span> <span class="k">return</span> <span class="bp">self</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">idempotence_hash_</span> <span class="o">=</span> <span class="n">idempotence_hash</span>

		<span class="c1"># Sklearn does not allow mutation of object parameters (the ones not</span>
		<span class="c1"># prepended by an underscore), so construct or reassign weights</span>
		<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> \
			<span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">_example_weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
		<span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
			<span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;example_weights provided to the constructor&#39;</span> <span class="o">+</span>
					<span class="s1">&#39; have dimension &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span>
					<span class="s1">&#39;, which disagrees with the size of X: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">_example_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span>

		<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim_weights</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> \
			<span class="bp">self</span><span class="o">.</span><span class="n">out_dim_weights</span> <span class="o">==</span> <span class="s1">&#39;inverse-variance&#39;</span><span class="p">:</span>
			<span class="n">variances</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
			<span class="c1"># There is a problem if a variance for any column is zero, because</span>
			<span class="c1"># its relative scale will appear infinite. </span>
			<span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">variances</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># if all zeros, don&#39;t readjust weights</span>
				<span class="n">variances</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="c1"># Fill zeros with the max of variances, so the corresponding</span>
				<span class="c1"># columns have small weight and are not major determiners of loss.</span>
				<span class="n">variances</span><span class="p">[</span><span class="n">variances</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">variances</span><span class="p">)</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">_out_dim_weights</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">variances</span>
		<span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim_weights</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> \
			<span class="bp">self</span><span class="o">.</span><span class="n">out_dim_weights</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">_out_dim_weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
		<span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim_weights</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
			<span class="k">if</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;out_dim_weights provided to the constructor&#39;</span> <span class="o">+</span>
					<span class="s1">&#39; have dimension &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span>
					<span class="s1">&#39;, which disagrees with the width of Y: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">_out_dim_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim_weights</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">_random</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

		<span class="c1"># Now that input and output dimensions are known, parameters vectors</span>
		<span class="c1"># can be initialized. Vectors are always stored vertically. Use the random</span>
		<span class="c1"># state to initialize idempotently if a random state is set.</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">_alpha_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">)</span> <span class="c1"># p x r</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">)</span> <span class="c1"># d x r</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">_f_</span> <span class="o">=</span> <span class="p">[</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">*</span><span class="mi">0</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">)]</span> <span class="c1"># zero functions</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">_df_</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">)]</span> <span class="c1"># no derivatives yet</span>

		<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">):</span> <span class="c1"># for each term in the additive model</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">_fit_stage</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_level</span> <span class="o">==</span> <span class="s1">&#39;high&#39;</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">_backfit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
			<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_level</span> <span class="o">==</span> <span class="s1">&#39;medium&#39;</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">_backfit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
		<span class="k">return</span> <span class="bp">self</span></div>


	<span class="k">def</span><span class="w"> </span><span class="nf">_fit_stage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">fit_weights</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;A &quot;stage&quot; consists of a set of alpha_j, f_j, and beta_j parameters.</span>
<span class="sd">		Given the stages already fit, find the residual this stage should try</span>
<span class="sd">		to match, and perform alternating optimization until parameters have</span>
<span class="sd">		converged.</span>

<span class="sd">		Parameters</span>
<span class="sd">		----------</span>
<span class="sd">		X : array-like of shape (n_samples, n_features)</span>
<span class="sd">			The training input samples.</span>
<span class="sd">		Y : array-like, shape (n_samples, n_outputs)</span>
<span class="sd">			The target values.</span>
<span class="sd">		j : int</span>
<span class="sd">			The index of this stage in the additive model.</span>
<span class="sd">		fit_weights : boolean</span>
<span class="sd">			Whether to refit alpha_j or leave it unmodified.</span>
<span class="sd">		</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="c1"># projections P = X*Alphas, P_j = X*alpha_j</span>
		<span class="n">P</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># the n x r projections matrix</span>
		<span class="c1"># The residuals matrix is essentially the evaluation function separated</span>
		<span class="c1"># in to the contribution of this term vs all the others and then</span>
		<span class="c1"># algebraically solved for this term&#39;s contribution by subtracting the</span>
		<span class="c1"># rest of the sum from both sides.</span>
		<span class="n">R_j</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">([</span><span class="n">numpy</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_f_</span><span class="p">[</span><span class="n">t</span><span class="p">](</span><span class="n">P</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> 
			<span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">)</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">j</span><span class="p">])</span> <span class="c1"># the n x d residuals matrix</span>

		<span class="c1"># main alternating optimization loop</span>
		<span class="n">itr</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># iteration counter</span>
		<span class="c1"># Start off with dummy infinite losses to get the loop started because</span>
		<span class="c1"># no value of loss should be able to accidentally fall within epsilon of</span>
		<span class="c1"># a dummy value and cause the loop to prematurely terminate.</span>
		<span class="n">prev_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">inf</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">inf</span>
		<span class="n">p_j</span> <span class="o">=</span> <span class="n">P</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span> <span class="c1"># n x 1, the jth column of the projections matrix</span>
		<span class="c1"># Use the absolute value between loss and previous loss because we do</span>
		<span class="c1"># not want to terminate if the instability of parameters in the first</span>
		<span class="c1"># few iterations causes loss to momentarily increase.</span>
		<span class="k">while</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">prev_loss</span> <span class="o">-</span> <span class="n">loss</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps_stage</span> <span class="ow">and</span> <span class="n">itr</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_maxiter</span><span class="p">):</span>			
			<span class="c1"># To understand how to optimize each set of parameters assuming the</span>
			<span class="c1"># others remain constant, see math.pdf section 3.</span>
			
			<span class="c1"># find the f_j</span>
			<span class="n">beta_j_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_out_dim_weights</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="c1"># weighted beta</span>
			<span class="n">targets</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">R_j</span><span class="p">,</span> <span class="n">beta_j_w</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
					  <span class="n">numpy</span><span class="o">.</span><span class="n">inner</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">beta_j_w</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-9</span><span class="p">)</span>
			<span class="c1"># Fit the targets against projections.</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">_f_</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df_</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_2d</span><span class="p">(</span><span class="n">p_j</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">itr</span><span class="p">)</span>
			
			<span class="c1"># find beta_j</span>
			<span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_f_</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">p_j</span><span class="p">)</span> <span class="c1"># Find the n x 1 vector of function outputs.</span>
			<span class="n">f_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_example_weights</span><span class="o">*</span><span class="n">f</span> <span class="c1"># f weighted by examples</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">R_j</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">f_w</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">inner</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">f_w</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-9</span><span class="p">)</span>

			<span class="c1"># find alpha_j</span>
			<span class="k">if</span> <span class="n">fit_weights</span><span class="p">:</span>
				<span class="c1"># Find the part of the Jacobians that is common to all</span>
				<span class="n">J</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_df_</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">p_j</span><span class="p">)</span><span class="o">*</span><span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_example_weights</span><span class="p">)</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
				<span class="n">JTJ</span> <span class="o">=</span> <span class="n">J</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">J</span> <span class="c1"># &lt;- weights yet to be factored in</span>
				<span class="n">A</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_out_dim_weights</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">JTJ</span>
					<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>
				<span class="c1"># Collect all g_jk vectors in to a convenient matrix G_j</span>
				<span class="n">G_j</span> <span class="o">=</span> <span class="n">R_j</span> <span class="o">-</span> <span class="n">numpy</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_f_</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">p_j</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
				<span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="nb">sum</span><span class="p">([</span><span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_out_dim_weights</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span>
					<span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">J</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">G_j</span><span class="p">[:,</span> <span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>

				<span class="n">delta</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
					<span class="n">b</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">rcond</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
				<span class="c1"># TODO implement halving step if the loss doesn&#39;t decrease with</span>
				<span class="c1"># this update.</span>
				<span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha_</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta</span>
				<span class="c1"># normalize to avoid numerical drift</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">_alpha_</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">/</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>

			<span class="c1"># Recalculate the jth projection with new f_j and alpha_j</span>
			<span class="n">p_j</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha_</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
			
			<span class="c1"># Calculate mean squared error for this iteration</span>
			<span class="n">prev_loss</span> <span class="o">=</span> <span class="n">loss</span>
			<span class="c1"># Subtract updated contribution of the jth term to get the</span>
			<span class="c1"># difference between Y and the predictions. </span>
			<span class="n">diff</span> <span class="o">=</span> <span class="n">R_j</span> <span class="o">-</span> <span class="n">numpy</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_f_</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">p_j</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
			<span class="c1"># square the difference, multiply rows by example weights, multiply</span>
			<span class="c1"># columns by their weights, and sum to get the final loss</span>
			<span class="n">loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_example_weights</span><span class="p">,</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">_out_dim_weights</span><span class="p">)</span>
			<span class="n">itr</span> <span class="o">+=</span> <span class="mi">1</span>

	<span class="k">def</span><span class="w"> </span><span class="nf">_backfit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">fit_weights</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Backfitting is the process of refitting all stages after a new stage</span>
<span class="sd">		is found. The idea is that the new stage causes the residuals for other</span>
<span class="sd">		stages to change, so it may be possible to do better in fewer stages by</span>
<span class="sd">		accounting for this new information.</span>

<span class="sd">		Refitting occurs for one stage at a time, cyclically around the set</span>
<span class="sd">		until convergence. Refitting a stage is expensive, so backfitting can be</span>
<span class="sd">		extremely so. Use `backfit_maxiter` to limit the number of cycles.</span>

<span class="sd">		Parameters</span>
<span class="sd">		----------</span>
<span class="sd">		X : array-like of shape (n_samples, n_features)</span>
<span class="sd">			The training input samples.</span>
<span class="sd">		Y : array-like, shape (n_samples, n_outputs)</span>
<span class="sd">			The target values.</span>
<span class="sd">		j : int</span>
<span class="sd">			The index of this stage in the additive model.</span>
<span class="sd">		fit_weights : boolean</span>
<span class="sd">			Whether to refit stages&#39; alphas or leave them unmodified.</span>
<span class="sd">		</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">itr</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="n">prev_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">inf</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">inf</span>
		<span class="k">while</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">prev_loss</span> <span class="o">-</span> <span class="n">loss</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps_backfit</span> <span class="ow">and</span> <span class="n">itr</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">backfit_maxiter</span><span class="p">):</span>
			<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span><span class="p">):</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">_fit_stage</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">fit_weights</span><span class="p">)</span>

			<span class="n">prev_loss</span> <span class="o">=</span> <span class="n">loss</span>
			<span class="n">diff</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
			<span class="n">loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_example_weights</span><span class="p">,</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">_out_dim_weights</span><span class="p">)</span>
			<span class="n">itr</span> <span class="o">+=</span> <span class="mi">1</span>

	<span class="k">def</span><span class="w"> </span><span class="nf">_fit_2d</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">itr</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Find a function mapping from x points in R1 to y points in R1.</span>

<span class="sd">		Parameters</span>
<span class="sd">		----------</span>
<span class="sd">		x : array-like</span>
<span class="sd">			Input points.</span>
<span class="sd">		y : array-like</span>
<span class="sd">			Target points.</span>
<span class="sd">		j : int</span>
<span class="sd">			The index of the stage that requires this fit. Only used for plots.</span>
<span class="sd">		itr : int</span>
<span class="sd">			The current iteration of the alternating optimization process that</span>
<span class="sd">			requires this fit. Only used for plots.</span>

<span class="sd">		Returns</span>
<span class="sd">		-------</span>
<span class="sd">		fit : a callable requiring numerical input</span>
<span class="sd">		deriv : a callable requiring numerical input</span>

<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_type</span> <span class="o">==</span> <span class="s1">&#39;polyfit&#39;</span><span class="p">:</span>
			<span class="n">coeffs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
				<span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">deg</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span>
				<span class="n">w</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_example_weights</span><span class="p">)</span>
			<span class="n">fit</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)</span>
			<span class="n">deriv</span> <span class="o">=</span> <span class="n">fit</span><span class="o">.</span><span class="n">deriv</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_type</span> <span class="o">==</span> <span class="s1">&#39;spline&#39;</span><span class="p">:</span>
			<span class="n">order</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
			<span class="c1"># set s according to the recommendation at: stackoverflow.com/</span>
			<span class="c1"># questions/8719754/scipy-interpolate-univariatespline-not-</span>
			<span class="c1"># smoothing-regardless-of-parameters</span>
			<span class="n">fit</span> <span class="o">=</span> <span class="n">UnivariateSpline</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">order</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">order</span><span class="p">],</span> <span class="n">w</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_example_weights</span><span class="p">,</span>
				<span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">y</span><span class="o">.</span><span class="n">var</span><span class="p">(),</span> <span class="n">ext</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
			<span class="n">deriv</span> <span class="o">=</span> <span class="n">fit</span><span class="o">.</span><span class="n">derivative</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

		<span class="c1"># Plot the projections versus the residuals in matplotlib so the user</span>
		<span class="c1"># can get a picture of what is happening.</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">show_plots</span> <span class="ow">and</span> <span class="p">(</span><span class="n">itr</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">plot_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
			<span class="n">pyplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
			<span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;stage &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; iteration &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">itr</span><span class="p">))</span>
			<span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;projections&#39;</span><span class="p">)</span>
			<span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;residuals&#39;</span><span class="p">)</span>
			<span class="n">xx</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
			<span class="n">yy</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
			<span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
			<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

		<span class="k">return</span> <span class="n">fit</span><span class="p">,</span> <span class="n">deriv</span></div>



<div class="viewcode-block" id="ProjectionPursuitClassifier">
<a class="viewcode-back" href="../skpp.html#skpp.ProjectionPursuitClassifier">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ProjectionPursuitClassifier</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
<span class="w">	</span><span class="sd">&quot;&quot;&quot;Perform classification with projection pursuit.</span>

<span class="sd">	Parameters</span>
<span class="sd">	----------</span>

<span class="sd">	pairwise_loss_matrix : array-like of dimension (n_classes, n_classes), default=None</span>
<span class="sd">		The adjacency matrix L has entries L[c,k]=l_ck specifying the weight of</span>
<span class="sd">		the penalty of predicting the answer is class k when it is actually</span>
<span class="sd">		class c. If unspecified, all penalties are considered to have the same</span>
<span class="sd">		importance.</span>
<span class="sd">		</span>

<span class="sd">	See Also</span>
<span class="sd">	--------</span>
<span class="sd">	ProjectionPursuitRegressor : for definitions of other parameters</span>
<span class="sd">	</span>
<span class="sd">	&quot;&quot;&quot;</span>
	<span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fit_type</span><span class="o">=</span><span class="s1">&#39;polyfit&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">opt_level</span><span class="o">=</span><span class="s1">&#39;high&#39;</span><span class="p">,</span>
				 <span class="n">example_weights</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">pairwise_loss_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
				 <span class="n">eps_stage</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">eps_backfit</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">stage_maxiter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
				 <span class="n">backfit_maxiter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_plots</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
				 <span class="n">plot_epoch</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>

		<span class="c1"># Do parameter checking for parameters that will not be checked when</span>
		<span class="c1"># the inner PPR model is constructed.</span>
		<span class="k">if</span> <span class="n">pairwise_loss_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="k">try</span><span class="p">:</span>
				<span class="n">pairwise_loss_matrix</span> <span class="o">=</span> <span class="n">as_float_array</span><span class="p">(</span><span class="n">pairwise_loss_matrix</span><span class="p">)</span>
			<span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">)</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;pairwise_loss_matrix must be None or array-like.&#39;</span><span class="p">)</span>
			<span class="k">if</span> <span class="n">numpy</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pairwise_loss_matrix</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;pairwise_loss_matrix can not contain negatives.&#39;</span><span class="p">)</span>
			<span class="k">elif</span> <span class="n">numpy</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">pairwise_loss_matrix</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;pairwise_loss_matrix[i,i] must == 0 for all i.&#39;</span><span class="p">)</span>

		<span class="c1"># sklearn&#39;s clone() works by calling get_params, which calls get_param_</span>
		<span class="c1"># names to crawl the constructor and find out which parameters are</span>
		<span class="c1"># necessary to reconstruct the model without its data, and then calling</span>
		<span class="c1"># getattr a bunch of times to find the settings of these parameters in</span>
		<span class="c1"># the object. So if you simply use the parameters and don&#39;t actually</span>
		<span class="c1"># save them as attributes, clone will pass a bunch of Nones rather than</span>
		<span class="c1"># the defaults to the constructor.</span>
		<span class="n">params</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
			<span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s1">&#39;self&#39;</span><span class="p">:</span>
				<span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

<div class="viewcode-block" id="ProjectionPursuitClassifier.fit">
<a class="viewcode-back" href="../skpp.html#skpp.ProjectionPursuitClassifier.fit">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Train the model.</span>

<span class="sd">		Parameters</span>
<span class="sd">		----------</span>
<span class="sd">		X : array-like of shape (n_samples, n_features)</span>
<span class="sd">			The training input samples.</span>
<span class="sd">		Y : array-like, shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">			The target values.</span>

<span class="sd">		Returns</span>
<span class="sd">		-------</span>
<span class="sd">		self : ProjectionPursuitClassifier:</span>
<span class="sd">			A trained model.</span>
<span class="sd">		</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">Y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Need 2D Y for encoding purposes</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">n_features_in_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

		<span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> \
			<span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span> <span class="ow">or</span> \
			<span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> \
			<span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
			<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Examples can only be weighted as `uniform` or &#39;</span> <span class="o">+</span> \
				<span class="s1">&#39;given numerical weights where len(example_weights)==n_examples.&#39;</span><span class="p">)</span>

		<span class="c1"># classes_ property is required for sklearn classifiers. unique_labels</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">unique_labels</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="c1"># also performs some input validation.</span>

		<span class="c1"># Encode the input Y as a multi-column H.</span>
		<span class="c1"># sparse_output=False until numpy fixes crazy sparse matrix dot() behavior</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">categories</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">sparse_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
		<span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

		<span class="c1"># Calculate the weights. See section 4 of math.pdf.</span>
		<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
			<span class="n">w_ex</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">pi_c</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># pi_c * n, technically, for all c</span>
			<span class="n">s_c</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span> <span class="c1"># find s_c for all c</span>
			<span class="n">w_ex</span> <span class="o">=</span> <span class="n">s_c</span> <span class="o">/</span> <span class="p">(</span><span class="n">pi_c</span> <span class="o">+</span> <span class="mf">1e-9</span><span class="p">)</span> <span class="c1"># column weights due to example weights</span>

		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_loss_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">w_pl</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pairwise_loss_matrix</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># sum over k</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">w_pl</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

		<span class="c1"># The problem is reduced to regression</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">_ppr_</span> <span class="o">=</span> <span class="n">ProjectionPursuitRegressor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_type</span><span class="p">,</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_level</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_weights</span><span class="p">,</span> <span class="n">w_ex</span><span class="o">*</span><span class="n">w_pl</span><span class="p">,</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">eps_stage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps_backfit</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_maxiter</span><span class="p">,</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">backfit_maxiter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">show_plots</span><span class="p">,</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">plot_epoch</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">_ppr_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
		<span class="k">return</span> <span class="bp">self</span></div>


<div class="viewcode-block" id="ProjectionPursuitClassifier.predict">
<a class="viewcode-back" href="../skpp.html#skpp.ProjectionPursuitClassifier.predict">[docs]</a>
	<span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">		</span><span class="sd">&quot;&quot;&quot;Use the fitted estimator to make predictions on new data.</span>

<span class="sd">		Parameters</span>
<span class="sd">		----------</span>
<span class="sd">		X : array-like of shape (n_samples, n_features)</span>
<span class="sd">			The input samples.</span>

<span class="sd">		Returns</span>
<span class="sd">		-------</span>
<span class="sd">		Y : array of shape (n_samples)</span>
<span class="sd">			The result of passing X through the evaluation function, taking</span>
<span class="sd">			the argmax of the output, and mapping it back to a class.</span>
<span class="sd">		</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_ppr_&#39;</span><span class="p">)</span>
		<span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ppr_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">H</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># OneHotEncoder needs 2D</span>
			<span class="n">H</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

		<span class="c1"># Encoder does the argmax over columns itself to find the index of the</span>
		<span class="c1"># most likely class then maps back to the class itself.</span>
		<span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># because sklearn&#39;s tests want a 1D output given</span>
			<span class="k">return</span> <span class="n">H</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>		<span class="c1"># a 1D input. Kind of pedantic.</span>
		<span class="k">return</span> <span class="n">H</span></div>
</div>

 
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">projection-pursuit</a></h1>



<p class="blurb">Machine learning through projection pursuit.</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=pavelkomarov&repo=projection-pursuit&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>






<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2021, Pavel Komarov.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
    </div>

    

    
  </body>
</html>